{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DANCES: Run models on YouTube video",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "v0roB48YXIll",
        "63BJgpQLcn_C",
        "kSs0LZYYsyeA",
        "3eiSQHNSXNYf"
      ],
      "authorship_tag": "ABX9TyMu7I4XBMCAn98dWEWvLMJd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindita/dances/blob/main/run_dances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0roB48YXIll"
      },
      "source": [
        "# Step 0: Set up HRNet utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjKRYD0TW_KN"
      },
      "source": [
        "!git clone https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agb4hmsbXRgl"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd deep-high-resolution-net.pytorch/\n",
        "pip install -r requirements.txt\n",
        "\n",
        "cd lib\n",
        "make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psbOpkLzXTaO"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSbLLphjXU2s"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd cocoapi/PythonAPI\n",
        "make install\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQPqJ9YrXWMm"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd deep-high-resolution-net.pytorch/\n",
        "mkdir output \n",
        "mkdir log\n",
        "mkdir data\n",
        "\n",
        "cd data && mkdir mpii\n",
        "cd mpii && mkdir images && mkdir annot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4s6WPvAUoLb"
      },
      "source": [
        "# %%bash\n",
        "\n",
        "# cd deep-high-resolution-net.pytorch/output\n",
        "# mkdir mpii && cd mpii\n",
        "# mkdir pose_hrnet\n",
        "# mkdir pose_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63BJgpQLcn_C"
      },
      "source": [
        "# Step 0.5: Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZEgEuhAaiMX"
      },
      "source": [
        "# Variables for 2570\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "video_size = (320, 240)\n",
        "init_scale = '0.7'\n",
        "scale = '0.6'\n",
        "\n",
        "dataset = '2570'\n",
        "\n",
        "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
        "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
        "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IVkzjEdUUWe"
      },
      "source": [
        "# Variables for 3000\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "\n",
        "# video_size = (1280, 720)\n",
        "# scale = '3'\n",
        "\n",
        "# dataset = '3000'\n",
        "\n",
        "# colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
        "#           [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
        "#           [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSs0LZYYsyeA"
      },
      "source": [
        "# Step 1: Mount drive & download video (see cells)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZUaQmoIz-X3"
      },
      "source": [
        "Please ensure you have added the `diving_models` [Drive folder](https://drive.google.com/drive/folders/1nDXZKhvn9TsEjeT6d2pdKsxB2afQqBsp?usp=sharing) to your `MyDrive` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGG9VzYoTX41"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGqm15QWVoS8"
      },
      "source": [
        "YOUTUBE_ID = 'F_mmLMTFMag'\n",
        "\n",
        "!pip install -q youtube-dl\n",
        "!rm youtube.mp4\n",
        "!youtube-dl -f 'bestvideo[ext=mp4]' --output \"youtube.%(ext)s\" https://www.youtube.com/watch?v=$YOUTUBE_ID\n",
        "!ffmpeg -y -loglevel info -i youtube.mp4 -ss 00:00:41 -t 2 video.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eiSQHNSXNYf"
      },
      "source": [
        "# Step 2: Create images from video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYLdjsghToKm"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "cam = cv2.VideoCapture(\"video.mp4\")\n",
        "currentframe = 0\n",
        "  \n",
        "while(True):\n",
        "    ret,frame = cam.read()\n",
        "  \n",
        "    if ret:\n",
        "        name = '/content/deep-high-resolution-net.pytorch/data/mpii/images/frame' + str(currentframe) + '.jpg'\n",
        "        print ('Creating...' + name)\n",
        "  \n",
        "        frame = cv2.resize(frame, (1280,720))\n",
        "        frame = frame[0:720, 160:1120]\n",
        "        frame = cv2.resize(frame, video_size)\n",
        "        cv2.imwrite(name, frame)\n",
        "\n",
        "        currentframe += 1\n",
        "    else:\n",
        "        break\n",
        "  \n",
        "cam.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xToPzbyZZSpH"
      },
      "source": [
        "!cp --verbose -R /content/drive/MyDrive/diving_models /content/deep-high-resolution-net.pytorch/\n",
        "\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "valid = []\n",
        "index = 0\n",
        "\n",
        "with open('deep-high-resolution-net.pytorch/diving_models/sample.json') as json_data:\n",
        "  data = json.load(json_data)\n",
        "  img = data[0]\n",
        "  for i in range(currentframe):\n",
        "    new_img = img.copy()\n",
        "    new_img[\"image\"] = 'frame' + str(i) + '.jpg'\n",
        "    new_img[\"scale\"] = scale\n",
        "    valid.append(new_img)\n",
        "\n",
        "  for i in range(3):\n",
        "    valid[i][\"scale\"] = init_scale\n",
        "    valid[-i][\"scale\"] = init_scale\n",
        "\n",
        "with open('deep-high-resolution-net.pytorch/data/mpii/annot/valid.json', 'w') as f:\n",
        "    json.dump(valid, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FThXFs64btHX"
      },
      "source": [
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "gt_dict = loadmat('deep-high-resolution-net.pytorch/diving_models/sample.mat')\n",
        "dataset_joints = gt_dict['dataset_joints']\n",
        "file_name = 'deep-high-resolution-net.pytorch/data/mpii/annot/valid.json'\n",
        "\n",
        "## Missing joint data\n",
        "\n",
        "with open(file_name) as json_data:\n",
        "  data = json.load(json_data)\n",
        "\n",
        "  joint_missing = [[] for joint in data[0]['joints_vis']]\n",
        "\n",
        "  for i,img in enumerate(data):\n",
        "    for j,jnt in enumerate(img['joints_vis']):\n",
        "      if jnt == 0:\n",
        "        joint_missing[j].append(1)\n",
        "      else:\n",
        "        joint_missing[j].append(0)\n",
        "\n",
        "## Positive ground truth sources\n",
        "\n",
        "with open(file_name) as json_data:\n",
        "  data = json.load(json_data)\n",
        "\n",
        "  ground_truth = [[[] for j in jnt] for jnt in data[0]['joints']]\n",
        "\n",
        "  for i,img in enumerate(data):\n",
        "    for j,jnt in enumerate(img['joints']): # 16 joints\n",
        "      ground_truth[j][0].append(jnt[0])\n",
        "      ground_truth[j][1].append(jnt[1])\n",
        "\n",
        "## Headboxes\n",
        "\n",
        "import math\n",
        "\n",
        "with open(file_name) as json_data:\n",
        "  data = json.load(json_data)\n",
        "\n",
        "  heads = [[[] for coord in range(2)] for jnt in range(2)]\n",
        "  # bttm l x, bttm l y, top r x, top r y\n",
        "\n",
        "  for i,img in enumerate(data):\n",
        "    neck_x = img['joints'][8][0]\n",
        "    neck_y = img['joints'][8][1]\n",
        "    head_x = img['joints'][9][0]\n",
        "    head_y = img['joints'][9][1]\n",
        "\n",
        "    half = math.sqrt((neck_x - head_x) ** 2 + (neck_y - head_y) ** 2) / 2\n",
        "\n",
        "    mid_x = (neck_x + head_x) / 2\n",
        "    mid_y = (neck_y + head_y) / 2\n",
        "\n",
        "    bml_x = int(min(mid_x - half, neck_x, head_x))\n",
        "    bml_y = int(min(mid_y - half, neck_y, head_y))\n",
        "    tpr_x = int(max(mid_x + half, neck_x, head_x))\n",
        "    tpr_y = int(max(mid_y + half, neck_y, head_y))\n",
        "\n",
        "    heads[0][0].append(bml_x)\n",
        "    heads[0][1].append(bml_y)\n",
        "    heads[1][0].append(tpr_x)\n",
        "    heads[1][1].append(tpr_y)\n",
        "\n",
        "\n",
        "savemat(\"deep-high-resolution-net.pytorch/data/mpii/annot/gt_valid.mat\", mdict={'dataset_joints': dataset_joints, 'jnt_missing': joint_missing, 'pos_gt_src': ground_truth, 'headboxes_src': heads})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN17gf4bh7G_"
      },
      "source": [
        "# Step 3: Run Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcOlPBcV5tau"
      },
      "source": [
        "!pip install yacs\n",
        "!pip install json_tricks\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nfwX3U4DeRS"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cp --verbose /content/deep-high-resolution-net.pytorch/diving_models/test.py /content/deep-high-resolution-net.pytorch/tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcOyKZugh-wK"
      },
      "source": [
        "def run_test(model_name, cfg='experiments/mpii/resnet/res101_256x256_d256x3_adam_lr1e-3.yaml', out='resnet/res101_256x256_d256x3_adam_lr1e-3', backbone='resnet'):\n",
        "  \n",
        "  best_path = f'/content/deep-high-resolution-net.pytorch/diving_models/{model_name}_model_best.pth'\n",
        "  final_path = f'/content/deep-high-resolution-net.pytorch/diving_models/{model_name}_final_state.pth'\n",
        "\n",
        "  !cd /content/deep-high-resolution-net.pytorch && python tools/test.py --cfg $cfg TEST.MODEL_FILE $best_path\n",
        "\n",
        "  matlab = f'/content/deep-high-resolution-net.pytorch/output/mpii/pose_{out}/pred.mat'\n",
        "  best_mat = f'/content/{model_name}_{scale}_best.mat'\n",
        "  !mv $matlab $best_mat\n",
        "\n",
        "  !cd /content/deep-high-resolution-net.pytorch && python tools/test.py --cfg $cfg TEST.MODEL_FILE $final_path\n",
        "  final_mat = f'/content/{model_name}_{scale}_final.mat'\n",
        "  !mv $matlab $final_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMKUW5qj5tIf"
      },
      "source": [
        "backbone = 'hrnet'\n",
        "out = 'w32_256x256_adam_lr1e-3'\n",
        "run_test(f'w32_{dataset}', f'experiments/mpii/{backbone}/{out}.yaml', f'{backbone}/{out}', backbone)\n",
        "out = 'w48_256x256_adam_lr1e-3'\n",
        "run_test(f'w48_{dataset}', f'experiments/mpii/{backbone}/{out}.yaml', f'{backbone}/{out}', backbone)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQkvT8MbSlwX"
      },
      "source": [
        "# Step 4: Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-VYaE52FFu3"
      },
      "source": [
        "from scipy.io import loadmat, savemat\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json\n",
        "\n",
        "\n",
        "model_1 = loadmat(f'/content/w32_{dataset}_{scale}_best.mat')\n",
        "model_2 = loadmat(f'/content/w32_{dataset}_{scale}_final.mat')\n",
        "model_3 = loadmat(f'/content/w48_{dataset}_{scale}_best.mat')\n",
        "model_4 = loadmat(f'/content/w48_{dataset}_{scale}_final.mat')\n",
        "\n",
        "# The joints correspond to the model which has that joint's highest accuracy\n",
        "models = [model_1, model_2, model_3, model_4]\n",
        "joints = [1,2,2,2,2,1,2,1,1,1,3,1,0,0,1,3]\n",
        "\n",
        "out = cv2.VideoWriter('model_output.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, video_size)\n",
        "results = []\n",
        "\n",
        "for i in range(len(model_1['preds'])):\n",
        "\n",
        "  frame_res = []\n",
        "  img = cv2.imread(f'/content/deep-high-resolution-net.pytorch/data/mpii/images/frame{i}.jpg')\n",
        "\n",
        "  for j in range(len(model_1['preds'][0])):\n",
        "    model = models[joints[j]]\n",
        "    x = int(model['preds'][i][j][0])\n",
        "    y = int(model_1['preds'][i][j][1])\n",
        "    cv2.circle(img, (x, y), 3, colors[j], -1)\n",
        "    frame_res.append([x,y])\n",
        "\n",
        "  cv2_imshow(img)\n",
        "  out.write(img)\n",
        "  results.append(frame_res)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "out.release()\n",
        "\n",
        "with open('data.json', 'w') as f:\n",
        "    json.dump(results, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGFxKu3qlzuP"
      },
      "source": [
        "# Step 4.5: Kalman Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TJH74QkjeVQ"
      },
      "source": [
        "!pip install tsmoothie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA-rs6mnruAp"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tsmoothie.utils_func import sim_randomwalk\n",
        "from tsmoothie.smoother import *\n",
        "\n",
        "p = 16\n",
        "\n",
        "with open('data.json') as json_file:\n",
        "  loaded = json.load(json_file)\n",
        "  scaled = np.array([[[x * 3 for x in y] for y in z] for z in loaded])\n",
        "\n",
        "def smooth(p):\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for keypoints in scaled:\n",
        "    sing = np.ravel(keypoints[p])\n",
        "    xs.append(sing[0])\n",
        "    ys.append(sing[1])\n",
        "  data = [xs,ys]\n",
        "  data = np.array(data)\n",
        "\n",
        "  # Kalman Smoothing\n",
        "  smoother = KalmanSmoother(component='level_trend', \n",
        "                            component_noise={'level':0.5, 'trend':1})\n",
        "  smoother.smooth(data)\n",
        "  low, up = smoother.get_intervals('kalman_interval', confidence=0.05)\n",
        "\n",
        "\n",
        "  # plot the first smoothed timeseries with intervals\n",
        "  # plt.figure(figsize=(11,6))\n",
        "  # plt.plot(smoother.smooth_data[0], linewidth=3, color='blue')\n",
        "  # plt.plot(smoother.data[0], '.k')\n",
        "  # plt.fill_between(range(len(smoother.data[0])), low[0], up[0], alpha=0.1)\n",
        "\n",
        "  return smoother.smooth_data\n",
        "\n",
        "\n",
        "smooths = []\n",
        "\n",
        "for i in range(p):\n",
        "  smooths.append(smooth(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffe1XxRDrxKC"
      },
      "source": [
        "\n",
        "out = cv2.VideoWriter('kalman_video.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 10, (960,720))\n",
        "results = []\n",
        "name = 'kalman'\n",
        "\n",
        "for i, keypoints in enumerate(scaled):\n",
        "  frame = cv2.imread(f'/content/deep-high-resolution-net.pytorch/data/mpii/images/frame{i}.jpg')\n",
        "  frame_res = []\n",
        "  for j in range(p):\n",
        "    #cv2.circle(frame, (int(keypoints[j][0]), (int(keypoints[j][1]))), 5, colors[j], 2)\n",
        "    x = int(smooths[j][0][i] / 3)\n",
        "    y = int(smooths[j][1][i] / 3)\n",
        "    cv2.circle(frame, (x, y), 3, colors[j], -1)\n",
        "    frame_res.append([x,y])\n",
        "  results.append(frame_res)\n",
        "  out.write(frame)\n",
        "  cv2_imshow(frame)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "out.release()\n",
        "\n",
        "with open(f'{name}.json', 'w') as f:\n",
        "    json.dump(results, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GojHMzwufQCu"
      },
      "source": [
        "# Step 5: Optical Flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnDX2R3afRuM"
      },
      "source": [
        "from scipy.io import loadmat, savemat\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "num_imgs = len(smooths[0][0])\n",
        "num_joints = len(smooths)\n",
        "\n",
        "# Parameters for lucas kanade optical flow\n",
        "lk_params = dict( winSize  = (10,10),\n",
        "                  maxLevel = 2,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 5, 0.03))\n",
        "\n",
        "def get_keypoints(frame_num):\n",
        "  keypoints = []\n",
        "  for i in range(num_joints):\n",
        "    x = smooths[i][0][frame_num] / 3\n",
        "    y = smooths[i][1][frame_num] / 3\n",
        "    keypoints.append([[x,y]])\n",
        "  return np.array(keypoints, dtype=np.float32)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "def annot_img(img, keypoints):\n",
        "  for i, ps in enumerate(keypoints):\n",
        "    x,y = ps.ravel()\n",
        "    predictions.append([int(x),int(y)])\n",
        "    cv2.circle(img, (x, y), 3, colors[i], -1)\n",
        "\n",
        "def dist(x1,y1,x2,y2):\n",
        "  return math.hypot(x2 - x1, y2 - y1)\n",
        "\n",
        "def analyse_points(prev_ps, opt_ps, opt_st, curr_ps):\n",
        "  keypoints = []\n",
        "  for i, ps in enumerate(curr_ps):\n",
        "    x, y = ps.ravel()\n",
        "\n",
        "    if opt_st[i][0] == 0:\n",
        "      keypoints.append([[x,y]])\n",
        "      continue\n",
        "\n",
        "    prev_x, prev_y = prev_ps[i].ravel()\n",
        "    opt_x, opt_y = opt_ps[i].ravel()\n",
        "\n",
        "    curr_dist = dist(prev_x, prev_y, x, y)\n",
        "    opt_dist = dist(prev_x, prev_y, opt_x, opt_y)\n",
        "\n",
        "    if curr_dist - opt_dist < 46:\n",
        "      keypoints.append([[x,y]])\n",
        "    else:\n",
        "      keypoints.append([[opt_x, opt_y]])\n",
        "\n",
        "    #keypoints.append([[x,y]])\n",
        "\n",
        "  return np.array(keypoints, dtype=np.float32)\n",
        "\n",
        "\n",
        "p0 = get_keypoints(0)\n",
        "kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "out = cv2.VideoWriter('output_video.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, video_size)\n",
        "\n",
        "for i in range(1, num_imgs):\n",
        "\n",
        "  old_img = cv2.imread(f'/content/deep-high-resolution-net.pytorch/data/mpii/images/frame{i-1}.jpg')\n",
        "  img = cv2.imread(f'/content/deep-high-resolution-net.pytorch/data/mpii/images/frame{i}.jpg')\n",
        "\n",
        "  old_gray = cv2.cvtColor(old_img, cv2.COLOR_BGR2GRAY)\n",
        "  old_gray = cv2.filter2D(old_gray, -1, kernel)\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  img_gray = cv2.filter2D(img_gray, -1, kernel)\n",
        "\n",
        "  p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, img_gray, p0, None, **lk_params)\n",
        "\n",
        "  curr_points = get_keypoints(i)\n",
        "  new_keypoints = analyse_points(p0, p1, st, curr_points)\n",
        "  annot_img(img, new_keypoints)\n",
        "\n",
        "  p0 = new_keypoints.copy()\n",
        "  \n",
        "  cv2_imshow(img)\n",
        "  out.write(img)\n",
        "  \n",
        "cv2.destroyAllWindows()\n",
        "out.release()\n",
        "\n",
        "\n",
        "with open('final_points.json', 'w') as f:\n",
        "    json.dump(predictions, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvfPVF3C-s2q"
      },
      "source": [
        "# Optional: Zip Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2qg16qY-zvY"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"images\",'zip',\"/content/deep-high-resolution-net.pytorch/data/mpii/images\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}